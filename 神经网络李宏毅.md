# machineleaning-work-log  
资料汇总  
https://blog.csdn.net/weixin_44406200/article/details/106423355  
https://github.com/Sakura-gh/  
  
作业  
https://github.com/Sakura-gh/ML-assignments/  
第一题  
https://www.freesion.com/article/6038906141/  
  
第二题  
https://blog.csdn.net/iteapoy/article/details/105477848  
问题，无法优化到0.89  
https://blog.csdn.net/iteapoy/article/details/105477848?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight  
这篇没看，据说可以到0.9  
https://www.cnblogs.com/HL-space/p/10785225.html  

1，梯度下降的方向，就是法线的方向，是和曲线垂直的  
2，步长必须是一个，这样保证梯度下降的方向不变，只改变长度  
正则化怎么理解  
l2是损失函数加上λΣw2   
l1是损失函数加上λΣ|w|  
目的是降低高次项函数的波动  

3，关于训练如果步长太大，不能收敛，如果步长太小，要看是否方向正确，并增加训练次数  

4，模型的误差来源及减少误差的方法  
如果实际error主要来自于variance很大，这个状况就是overfitting过拟合；如果实际error主要来自于bias很大，这个状况就是underfitting欠拟合  
如果model没有办法fit training data的examples，代表bias比较大，这时是underfitting  
如果model可以fit training data，在training data上得到小的error，但是在testing data上，却得到一个大的error，代表variance比较大，这时是overfitting  
    1、如果bias比较大  
    redesign，重新设计你的model  
    增加更多的features作为model的input输入变量  
    让model变得更复杂，增加高次项  
    2、如果variance比较大  
    增加data  
    Regularization(正规化)  
6,如果一个两分类问题，不能用线性回归来进行优化，一是不能积分，二是最小二乘法计算误差不好用。如果有相同的方差，那么可以利用一个sigmod函数转换为一个线性回归问题  
7,分类问题用交叉熵计算误差效果比较好，用最小二乘法，梯度下降的速度比较慢。逻辑回归分为二种，一种是Generative的方法，另一种是Discriminative。generative假设符合某种分布，Discriminative不做任何假设。  

第八章Deep Learning  
任何连续的function，它input是一个N维的vector，output是一个M维的vector，它都可以用一个hidden layer的neural network来表示，只要你这个hidden layer的neuron够多，它可以表示成任何的function  

作业1,用到adagrad，动量等等  
https://zhuanlan.zhihu.com/p/147275344  
部分公式推倒  
https://blog.csdn.net/weixin_42447868/article/details/105261672  

10 Keras2.0  
batch size 和epoch 的选择  
一般来说 batch size 不能太小，这样的话速度太慢，不能发挥多核的优势  
同样来说 batch size 也不能太大，这样缺少了随机性，会陷入到鞍点或者局部的最小值当中  

11 cnn 1  
里面如何训练cnn 还有点疑问  

12 cnn 2  
没有看懂  

13 Tips for Deep Learning  
deep 训练时候遇见的问题，参数，层数较多的模型，效果不如层数少的模型。原因不是过拟合，而是数据不足，是梯度消失。模型遇见了最优质，所谓的鞍点，  
解决方法之一，采用其他的梯度下降算法。  
解决的方法之一是，用relu来替换sigmod  
这篇里面还有关于正则化，dagrad、RMSProp、Momentum、Adam的论述  

14 why deep  
如何识别语音还没有看懂  

