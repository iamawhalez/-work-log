# machineleaning-work-log
资料汇总
https://blog.csdn.net/weixin_44406200/article/details/106423355
https://github.com/Sakura-gh/

作业
https://github.com/Sakura-gh/ML-assignments/
第一题
https://www.freesion.com/article/6038906141/


1，梯度下降的方向，就是法线的方向，是和曲线垂直的
2，步长必须是一个，这样保证梯度下降的方向不变，只改变长度
正则化怎么理解
l2是损失函数加上λΣw2 
l1是损失函数加上λΣ|w|
目的是降低高次项函数的波动

3，关于训练如果步长太大，不能收敛，如果步长太小，要看是否方向正确，并增加训练次数

4，模型的误差来源及减少误差的方法
如果实际error主要来自于variance很大，这个状况就是overfitting过拟合；如果实际error主要来自于bias很大，这个状况就是underfitting欠拟合
如果model没有办法fit training data的examples，代表bias比较大，这时是underfitting
如果model可以fit training data，在training data上得到小的error，但是在testing data上，却得到一个大的error，代表variance比较大，这时是overfitting
    1、如果bias比较大
    redesign，重新设计你的model
    增加更多的features作为model的input输入变量
    让model变得更复杂，增加高次项
    2、如果variance比较大
    增加data
    Regularization(正规化)
6,如果一个两分类问题，不能用线性回归来进行优化，一是不能积分，二是最小二乘法计算误差不好用。如果有相同的方差，那么可以利用一个sigmod函数转换为一个线性回归问题
7,分类问题用交叉熵计算误差效果比较好，用最小二乘法，梯度下降的速度比较慢。逻辑回归分为二种，一种是Generative的方法，另一种是Discriminative。generative假设符合某种分布，Discriminative不做任何假设。


作业1,用到adagrad，动量等等
https://zhuanlan.zhihu.com/p/147275344
部分公式推倒
https://blog.csdn.net/weixin_42447868/article/details/105261672
